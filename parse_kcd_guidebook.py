"""
ì§ˆë³‘ì½”ë”©ì§€ì¹¨ì„œ PDF íŒŒì‹±
- PDFë¥¼ 20í˜ì´ì§€ì”© ë¶„í• í•˜ì—¬ Upstage APIë¡œ íŒŒì‹±
"""
import sys
import codecs
from pathlib import Path
import json
import os
from datetime import datetime
from dotenv import load_dotenv
from pypdf import PdfReader, PdfWriter
import requests
import time

# UTF-8 ì¶œë ¥
if sys.platform == 'win32':
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()


class PDFSplitParser:
    """PDF ë¶„í•  íŒŒì„œ"""

    def __init__(self, output_dir: str = 'data/hira_master/parsed', chunk_pages: int = 50):
        """
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            chunk_pages: í•œ ë²ˆì— ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜
        """
        self.api_key = os.getenv('UPSTAGE_API_KEY')
        if not self.api_key:
            raise ValueError('UPSTAGE_API_KEY not found in .env')

        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.chunk_pages = chunk_pages
        self.base_url = "https://api.upstage.ai/v1/document-ai/document-parse"

        # ì„ì‹œ ë¶„í•  íŒŒì¼ ì €ì¥ ìœ„ì¹˜
        self.temp_dir = self.output_dir / 'temp_chunks'
        self.temp_dir.mkdir(exist_ok=True)

    def split_pdf(self, pdf_path: Path) -> list:
        """PDFë¥¼ ì—¬ëŸ¬ íŒŒì¼ë¡œ ë¶„í• 

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ

        Returns:
            ë¶„í• ëœ PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        print(f'\nğŸ“„ PDF ë¶„í• : {pdf_path.name}')

        reader = PdfReader(pdf_path)
        total_pages = len(reader.pages)

        print(f'  ì´ í˜ì´ì§€: {total_pages}p')
        print(f'  ë¶„í•  ë‹¨ìœ„: {self.chunk_pages}p')
        print(f'  ì˜ˆìƒ íŒŒì¼: {(total_pages + self.chunk_pages - 1) // self.chunk_pages}ê°œ')

        chunks = []

        for start_page in range(0, total_pages, self.chunk_pages):
            end_page = min(start_page + self.chunk_pages, total_pages)

            # ìƒˆ PDF ìƒì„±
            writer = PdfWriter()
            for page_num in range(start_page, end_page):
                writer.add_page(reader.pages[page_num])

            # ì €ì¥
            chunk_filename = f'{pdf_path.stem}_pages_{start_page+1:04d}-{end_page:04d}.pdf'
            chunk_path = self.temp_dir / chunk_filename
            writer.write(chunk_path)

            chunks.append(chunk_path)
            print(f'  âœ… {chunk_filename} ({end_page - start_page}p)')

        return chunks

    def parse_chunk(self, chunk_path: Path, output_format: str = "html") -> dict:
        """ë‹¨ì¼ ì²­í¬ íŒŒì‹±"""
        print(f'\n[íŒŒì‹±] {chunk_path.name}')

        with open(chunk_path, 'rb') as f:
            files = {'document': (chunk_path.name, f, 'application/pdf')}
            data = {
                'ocr': 'true',
                'output_formats': f'["{output_format}"]',
            }

            start_time = time.time()
            try:
                response = requests.post(
                    self.base_url,
                    headers={"Authorization": f"Bearer {self.api_key}"},
                    files=files,
                    data=data,
                    timeout=300
                )
                elapsed = time.time() - start_time

                if response.status_code != 200:
                    error_msg = f"API Error {response.status_code}: {response.text}"
                    print(f'  âŒ ì‹¤íŒ¨: {error_msg}')
                    return {'error': error_msg, 'chunk': str(chunk_path)}

                result = response.json()

                # content ì¶”ì¶œ
                content_dict = result.get('content', {})
                if isinstance(content_dict, dict):
                    content_text = content_dict.get(output_format, '')
                else:
                    content_text = str(content_dict)

                result['chunk_metadata'] = {
                    'chunk_file': str(chunk_path),
                    'elapsed_seconds': elapsed,
                    'content_length': len(content_text),
                    'pages': result.get('usage', {}).get('pages', 0)
                }

                print(f'  âœ… ì„±ê³µ: {elapsed:.1f}ì´ˆ, {len(content_text):,}ì')
                return result

            except Exception as e:
                print(f'  âŒ ì—ëŸ¬: {e}')
                return {'error': str(e), 'chunk': str(chunk_path)}

    def parse_chunks(self, chunk_paths: list) -> list:
        """ë¶„í• ëœ PDF íŒŒì¼ë“¤ íŒŒì‹±

        Args:
            chunk_paths: ë¶„í• ëœ PDF ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

        Returns:
            íŒŒì‹± ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        print(f'\nğŸ”„ ì²­í¬ íŒŒì‹±: {len(chunk_paths)}ê°œ')

        results = []

        for i, chunk_path in enumerate(chunk_paths, 1):
            print(f'\n[{i}/{len(chunk_paths)}]')
            result = self.parse_chunk(chunk_path)
            results.append(result)

            # Rate limit ë°©ì§€
            if i < len(chunk_paths):
                time.sleep(1)

        return results

    def merge_results(self, results: list, source_file: str, output_format: str = 'html') -> dict:
        """íŒŒì‹± ê²°ê³¼ ë³‘í•©

        Args:
            results: íŒŒì‹± ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            source_file: ì›ë³¸ íŒŒì¼ëª…
            output_format: ì¶œë ¥ í˜•ì‹

        Returns:
            ë³‘í•©ëœ ê²°ê³¼
        """
        print(f'\nğŸ”— ê²°ê³¼ ë³‘í•©')

        # ì—ëŸ¬ê°€ ì—†ëŠ” ê²°ê³¼ë§Œ í•„í„°ë§
        valid_results = [r for r in results if 'error' not in r]

        if not valid_results:
            print('  âŒ ëª¨ë“  ì²­í¬ íŒŒì‹± ì‹¤íŒ¨')
            return None

        # Content ë³‘í•©
        merged_content = []
        for r in valid_results:
            content_dict = r.get('content', {})
            if isinstance(content_dict, dict):
                content_text = content_dict.get(output_format, '')
            else:
                content_text = str(content_dict)
            merged_content.append(content_text)

        merged_content_str = '\n\n<hr>\n\n'.join(merged_content)

        # Elements ë³‘í•©
        merged_elements = []
        for r in valid_results:
            if 'elements' in r:
                merged_elements.extend(r['elements'])

        # ì´ í˜ì´ì§€ ìˆ˜
        total_pages = sum(r.get('chunk_metadata', {}).get('pages', 0) for r in valid_results)

        merged = {
            'source_file': source_file,
            'total_pages': total_pages,
            'chunks_parsed': len(valid_results),
            'chunks_failed': len(results) - len(valid_results),
            'content': merged_content_str,
            'elements': merged_elements,
            'output_format': output_format,
            'parsed_at': datetime.now().isoformat(),
            'chunks_metadata': [r.get('chunk_metadata') for r in valid_results]
        }

        print(f'  âœ… ë³‘í•© ì™„ë£Œ: {total_pages}p, {len(merged_content_str):,}ì')
        print(f'  âœ… Elements: {len(merged_elements)}ê°œ')

        return merged

    def cleanup_temp_files(self):
        """ì„ì‹œ íŒŒì¼ ì‚­ì œ"""
        print(f'\nğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬')

        for file in self.temp_dir.glob('*.pdf'):
            file.unlink()
            print(f'  ì‚­ì œ: {file.name}')

    def parse_large_pdf(self, pdf_path: Path, output_format: str = 'html') -> dict:
        """ëŒ€ìš©ëŸ‰ PDF ì „ì²´ íŒŒì‹± í”„ë¡œì„¸ìŠ¤

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            output_format: ì¶œë ¥ í˜•ì‹

        Returns:
            ë³‘í•©ëœ íŒŒì‹± ê²°ê³¼
        """
        print(f'\n{"="*80}')
        print(f'ğŸ“š ëŒ€ìš©ëŸ‰ PDF íŒŒì‹±: {pdf_path.name}')
        print(f'{"="*80}')

        # 1. PDF ë¶„í• 
        chunks = self.split_pdf(pdf_path)

        # 2. ê° ì²­í¬ íŒŒì‹±
        results = self.parse_chunks(chunks)

        # 3. ê²°ê³¼ ë³‘í•©
        merged = self.merge_results(results, pdf_path.name, output_format)

        # 4. ì €ì¥
        if merged:
            # JSON ì €ì¥
            output_file = self.output_dir / f'{pdf_path.stem}.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(merged, f, ensure_ascii=False, indent=2)

            print(f'\nğŸ’¾ JSON ì €ì¥: {output_file}')

            # Raw content ì €ì¥ (HTML)
            raw_file = self.output_dir / f'{pdf_path.stem}.{output_format}'
            with open(raw_file, 'w', encoding='utf-8') as f:
                f.write(merged['content'])

            print(f'ğŸ’¾ Raw ì €ì¥: {raw_file}')

            # ë¯¸ë¦¬ë³´ê¸°
            print(f'\n[{output_format.upper()} ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì)]')
            print('-' * 80)
            print(merged['content'][:500])
            print('-' * 80)

        # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        self.cleanup_temp_files()

        return merged


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print('=' * 80)
    print('ğŸ¥ ì§ˆë³‘ì½”ë”©ì§€ì¹¨ì„œ PDF íŒŒì‹±')
    print('=' * 80)

    # íŒŒì‹±í•  PDF
    pdf_path = Path('data/hira_master/ë¶™ì„1_2021ë…„+í•œêµ­í‘œì¤€ì§ˆë³‘ì‚¬ì¸ë¶„ë¥˜+ì§ˆë³‘ì½”ë”©ì§€ì¹¨ì„œ.pdf')

    if not pdf_path.exists():
        print(f'âŒ íŒŒì¼ ì—†ìŒ: {pdf_path}')
        return

    try:
        # 50í˜ì´ì§€ì”© ë¶„í•  (Upstage ì œí•œ: 100í˜ì´ì§€)
        parser = PDFSplitParser(chunk_pages=50)
        result = parser.parse_large_pdf(pdf_path, output_format='html')

        # ìµœì¢… ìš”ì•½
        print('\n\n' + '=' * 80)
        print('ğŸ“Š íŒŒì‹± ì™„ë£Œ')
        print('=' * 80)

        if result:
            print(f'âœ… ì´ í˜ì´ì§€: {result.get("total_pages")}p')
            print(f'âœ… ì²­í¬: {result.get("chunks_parsed")}ê°œ ì„±ê³µ, {result.get("chunks_failed")}ê°œ ì‹¤íŒ¨')
            print(f'âœ… Content: {len(result.get("content", "")):,}ì')
            print(f'âœ… Elements: {len(result.get("elements", []))}ê°œ')
        else:
            print('âŒ íŒŒì‹± ì‹¤íŒ¨')

    except Exception as e:
        print(f'âŒ ì „ì²´ ì—ëŸ¬: {e}')
        import traceback
        traceback.print_exc()

    print('\n' + '=' * 80)
    print('âœ… ì™„ë£Œ')
    print('=' * 80)


if __name__ == '__main__':
    main()
