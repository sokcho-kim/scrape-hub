"""ëŒ€ìš©ëŸ‰ PDF ë¶„í•  ë° íŒŒì‹±

ì „ëµ:
1. PDFë¥¼ 10-20í˜ì´ì§€ì”© ë¶„í• 
2. ê° ë¶€ë¶„ì„ Upstage APIë¡œ íŒŒì‹±
3. ê²°ê³¼ë¥¼ ë³‘í•©í•˜ì—¬ ì „ì²´ ë¬¸ì„œ ìƒì„±
"""
import sys
import codecs
from pathlib import Path
import json
import os
from datetime import datetime
from dotenv import load_dotenv
from pypdf import PdfReader, PdfWriter

sys.path.insert(0, str(Path(__file__).parent.parent))
from shared.parsers import UpstageParser

# UTF-8 ì¶œë ¥
if sys.platform == 'win32':
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()


class PDFSplitParser:
    """PDF ë¶„í•  íŒŒì„œ"""

    def __init__(self, output_dir: str = 'data/hira_master/parsed', chunk_pages: int = 20):
        """
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            chunk_pages: í•œ ë²ˆì— ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜
        """
        api_key = os.getenv('UPSTAGE_API_KEY')
        if not api_key:
            raise ValueError('UPSTAGE_API_KEY not found in .env')

        self.parser = UpstageParser(api_key=api_key)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.chunk_pages = chunk_pages

        # ì„ì‹œ ë¶„í•  íŒŒì¼ ì €ì¥ ìœ„ì¹˜
        self.temp_dir = self.output_dir / 'temp_chunks'
        self.temp_dir.mkdir(exist_ok=True)

    def split_pdf(self, pdf_path: Path) -> list[Path]:
        """PDFë¥¼ ì—¬ëŸ¬ íŒŒì¼ë¡œ ë¶„í• 

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ

        Returns:
            ë¶„í• ëœ PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        print(f'\nğŸ“„ PDF ë¶„í• : {pdf_path.name}')

        reader = PdfReader(pdf_path)
        total_pages = len(reader.pages)

        print(f'  ì´ í˜ì´ì§€: {total_pages}p')
        print(f'  ë¶„í•  ë‹¨ìœ„: {self.chunk_pages}p')
        print(f'  ì˜ˆìƒ íŒŒì¼: {(total_pages + self.chunk_pages - 1) // self.chunk_pages}ê°œ')

        chunks = []

        for start_page in range(0, total_pages, self.chunk_pages):
            end_page = min(start_page + self.chunk_pages, total_pages)

            # ìƒˆ PDF ìƒì„±
            writer = PdfWriter()
            for page_num in range(start_page, end_page):
                writer.add_page(reader.pages[page_num])

            # ì €ì¥
            chunk_filename = f'{pdf_path.stem}_pages_{start_page+1}-{end_page}.pdf'
            chunk_path = self.temp_dir / chunk_filename
            writer.write(chunk_path)

            chunks.append(chunk_path)
            print(f'  âœ… {chunk_filename} ({end_page - start_page}p)')

        return chunks

    def parse_chunks(self, chunk_paths: list[Path]) -> list[dict]:
        """ë¶„í• ëœ PDF íŒŒì¼ë“¤ íŒŒì‹±

        Args:
            chunk_paths: ë¶„í• ëœ PDF ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

        Returns:
            íŒŒì‹± ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        print(f'\nğŸ”„ ì²­í¬ íŒŒì‹±: {len(chunk_paths)}ê°œ')

        results = []

        for i, chunk_path in enumerate(chunk_paths, 1):
            print(f'\n[{i}/{len(chunk_paths)}] {chunk_path.name}')

            try:
                result = self.parser.parse(chunk_path)

                if result:
                    print(f'  âœ… ì„±ê³µ: {result.get("pages")}p, {len(result.get("content", ""))}ì')
                    results.append(result)
                else:
                    print(f'  âŒ ì‹¤íŒ¨')
                    results.append(None)

            except Exception as e:
                print(f'  âŒ ì—ëŸ¬: {e}')
                results.append(None)

        return results

    def merge_results(self, results: list[dict], source_file: str) -> dict:
        """íŒŒì‹± ê²°ê³¼ ë³‘í•©

        Args:
            results: íŒŒì‹± ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            source_file: ì›ë³¸ íŒŒì¼ëª…

        Returns:
            ë³‘í•©ëœ ê²°ê³¼
        """
        print(f'\nğŸ”— ê²°ê³¼ ë³‘í•©')

        # None í•„í„°ë§
        valid_results = [r for r in results if r is not None]

        if not valid_results:
            return None

        # Markdown ë³‘í•©
        merged_content = '\n\n---\n\n'.join([
            r.get('content', '') for r in valid_results
        ])

        # HTML ë³‘í•©
        merged_html = '\n<hr>\n'.join([
            r.get('html', '') for r in valid_results
        ])

        # ì´ í˜ì´ì§€ ìˆ˜
        total_pages = sum(r.get('pages', 0) for r in valid_results)

        merged = {
            'source_file': source_file,
            'total_pages': total_pages,
            'chunks_parsed': len(valid_results),
            'chunks_failed': len(results) - len(valid_results),
            'content': merged_content,
            'html': merged_html,
            'model': valid_results[0].get('model') if valid_results else None,
            'parsed_at': datetime.now().isoformat()
        }

        print(f'  âœ… ë³‘í•© ì™„ë£Œ: {total_pages}p, {len(merged_content)}ì')

        return merged

    def cleanup_temp_files(self):
        """ì„ì‹œ íŒŒì¼ ì‚­ì œ"""
        print(f'\nğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬')

        for file in self.temp_dir.glob('*.pdf'):
            file.unlink()
            print(f'  ì‚­ì œ: {file.name}')

    def parse_large_pdf(self, pdf_path: Path) -> dict:
        """ëŒ€ìš©ëŸ‰ PDF ì „ì²´ íŒŒì‹± í”„ë¡œì„¸ìŠ¤

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ

        Returns:
            ë³‘í•©ëœ íŒŒì‹± ê²°ê³¼
        """
        print(f'\n{"="*80}')
        print(f'ğŸ“š ëŒ€ìš©ëŸ‰ PDF íŒŒì‹±: {pdf_path.name}')
        print(f'{"="*80}')

        # 1. PDF ë¶„í• 
        chunks = self.split_pdf(pdf_path)

        # 2. ê° ì²­í¬ íŒŒì‹±
        results = self.parse_chunks(chunks)

        # 3. ê²°ê³¼ ë³‘í•©
        merged = self.merge_results(results, pdf_path.name)

        # 4. ì €ì¥
        if merged:
            output_file = self.output_dir / f'{pdf_path.stem}.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(merged, f, ensure_ascii=False, indent=2)

            print(f'\nğŸ’¾ ì €ì¥: {output_file}')

            # ë¯¸ë¦¬ë³´ê¸°
            print(f'\n[Markdown ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì)]')
            print('-' * 80)
            print(merged['content'][:500])
            print('-' * 80)

        # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        self.cleanup_temp_files()

        return merged


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print('=' * 80)
    print('ğŸ¥ HIRA ë§ˆìŠ¤í„° ë°ì´í„° PDF ë¶„í•  íŒŒì‹±')
    print('=' * 80)

    parser = PDFSplitParser(chunk_pages=20)  # 20í˜ì´ì§€ì”© ë¶„í• 

    # íŒŒì‹±í•  PDF ëª©ë¡
    master_dir = Path('data/hira_master')
    pdfs = [
        master_dir / 'KCD-8 1ê¶Œ_220630_20220630034856.pdf',
        master_dir / 'KDRG ë¶„ë¥˜ì§‘(ì‹ í¬ê´„ì§€ë¶ˆì œë„ìš© ver1.4).pdf',
    ]

    results_summary = {}

    for pdf_path in pdfs:
        if not pdf_path.exists():
            print(f'âš ï¸ íŒŒì¼ ì—†ìŒ: {pdf_path}')
            continue

        try:
            result = parser.parse_large_pdf(pdf_path)

            if result:
                results_summary[pdf_path.stem] = {
                    'pages': result.get('total_pages'),
                    'chunks': result.get('chunks_parsed'),
                    'failed': result.get('chunks_failed'),
                    'success': True
                }
            else:
                results_summary[pdf_path.stem] = {
                    'success': False,
                    'error': 'Parsing failed'
                }

        except Exception as e:
            print(f'âŒ ì „ì²´ ì—ëŸ¬: {e}')
            import traceback
            traceback.print_exc()
            results_summary[pdf_path.stem] = {
                'success': False,
                'error': str(e)
            }

    # ìµœì¢… ìš”ì•½
    print('\n\n' + '=' * 80)
    print('ğŸ“Š íŒŒì‹± ìš”ì•½')
    print('=' * 80)

    for name, info in results_summary.items():
        if info.get('success'):
            print(f'âœ… {name}:')
            print(f'   í˜ì´ì§€: {info.get("pages")}p')
            print(f'   ì²­í¬: {info.get("chunks")}ê°œ ì„±ê³µ, {info.get("failed")}ê°œ ì‹¤íŒ¨')
        else:
            print(f'âŒ {name}: {info.get("error")}')

    # í†µê³„ ì €ì¥
    summary_file = parser.output_dir / '_parsing_summary.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump({
            'parsed_at': datetime.now().isoformat(),
            'results': results_summary
        }, f, ensure_ascii=False, indent=2)

    print(f'\nğŸ’¾ ìš”ì•½ ì €ì¥: {summary_file}')

    print('\n' + '=' * 80)
    print('âœ… ì „ì²´ íŒŒì‹± ì™„ë£Œ')
    print('=' * 80)


if __name__ == '__main__':
    main()
