#!/usr/bin/env python3
"""
ê²Œì´íŠ¸ ì²´ì¸ ê¸°ë°˜ ì•½ì œ ë§¤ì¹­ ì •ì œ ìŠ¤í¬ë¦½íŠ¸

ëª©ì : drug_matching_results_v2.jsonì˜ ì˜¤íƒì„ ì œê±°í•˜ê³ ,
     ì •ë°€ë„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ë“œ ë ˆì´ì–´(ì•µì»¤ ì‚¬ì „)ë¥¼ êµ¬ì¶•

ì‘ì„±ì¼: 2025-10-30
"""

import json
import yaml
import re
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from collections import Counter, defaultdict
from dataclasses import dataclass, field, asdict
import unicodedata
from difflib import SequenceMatcher

# =============================================================================
# ë°ì´í„° í´ë˜ìŠ¤
# =============================================================================

@dataclass
class DrugEntry:
    """ì•½ì œ í•­ëª©"""
    en: str
    ko: str
    count: int = 0
    source: str = ""
    reason_codes: List[str] = field(default_factory=list)
    decision: str = "pending"  # active, pending, drop, route_*
    confidence: float = 0.0
    aliases: List[str] = field(default_factory=list)
    context_span: str = ""
    first_seen_source: str = ""

@dataclass
class GateChainStats:
    """ê²Œì´íŠ¸ ì²´ì¸ í†µê³„"""
    total_input: int = 0
    active: int = 0
    pending: int = 0
    dropped: int = 0
    routed_regimen: int = 0
    routed_biomarker: int = 0
    routed_disease: int = 0
    reason_code_counts: Counter = field(default_factory=Counter)

# =============================================================================
# ë©”ì¸ í´ë˜ìŠ¤
# =============================================================================

class DrugAnchorRefiner:
    """ì•½ì œ ì•µì»¤ ì •ì œê¸°"""

    def __init__(self, filters_path: str, brand_alias_path: Optional[str] = None, curated_pairs_path: Optional[str] = None):
        """
        ì´ˆê¸°í™”

        Args:
            filters_path: filters.yaml íŒŒì¼ ê²½ë¡œ
            brand_alias_path: brand_alias.yaml íŒŒì¼ ê²½ë¡œ (ì„ íƒ)
            curated_pairs_path: curated pairs JSON íŒŒì¼ ê²½ë¡œ (ì„ íƒ)
        """
        self.logger = logging.getLogger(__name__)

        # í•„í„° ê·œì¹™ ë¡œë“œ
        with open(filters_path, 'r', encoding='utf-8') as f:
            self.filters = yaml.safe_load(f)

        # ë¸Œëœë“œëª… ë§¤í•‘ ë¡œë“œ
        self.brand_to_ingredient = {}
        self.ingredient_to_brands = {}
        if brand_alias_path and Path(brand_alias_path).exists():
            with open(brand_alias_path, 'r', encoding='utf-8') as f:
                brand_data = yaml.safe_load(f)
                self.brand_to_ingredient = brand_data.get('brand_to_ingredient', {})
                self.ingredient_to_brands = brand_data.get('ingredient_to_brands', {})
            self.logger.info(f"Loaded {len(self.brand_to_ingredient)} brand mappings")

        # íë ˆì´ì…˜ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë¡œë“œ (ë‚˜ì¤‘ì— normalize_text ì‚¬ìš© í•„ìš”)
        self.curated_pairs_raw = []  # ì›ë³¸ ë°ì´í„° ì„ì‹œ ì €ì¥
        if curated_pairs_path and Path(curated_pairs_path).exists():
            with open(curated_pairs_path, 'r', encoding='utf-8-sig') as f:
                curated_data = json.load(f)

                # manual_drugs í˜•ì‹ ì²˜ë¦¬: {"manual_drugs": {"drug_en": ["ko1", "ko2"]}}
                if 'manual_drugs' in curated_data:
                    for en, ko_list in curated_data['manual_drugs'].items():
                        for ko in ko_list:
                            self.curated_pairs_raw.append({
                                'en': en,
                                'ko': ko
                            })

                # ë ˆê±°ì‹œ í˜•ì‹ ì²˜ë¦¬: {"matched_via_english": [...]}
                elif 'matched_via_english' in curated_data:
                    for item in curated_data.get('matched_via_english', []):
                        if item.get('source') == 'manual_curated':
                            self.curated_pairs_raw.append({
                                'en': item.get('english', ''),
                                'ko': item.get('korean', '')
                            })

        # ì»¨í…ìŠ¤íŠ¸ ì‹œê·¸ë„ í† í°
        self.ctx_tokens = [
            'íˆ¬ì—¬', 'ìš©ëŸ‰', 'mg/mÂ²', 'mg/kg', 'ì£¼ì‚¬', 'ì •ë§¥', 'ë³‘ìš©',
            '1ì¼', '2ì¼', '3ì¼', '1íšŒ', '2íšŒ', '3íšŒ',
            'ì‚¬ì´í´', 'cycle', 'ì£¼ê¸°',
            'ì„ìƒì‹œí—˜', 'ìœ íš¨ì„±', 'ì•ˆì „ì„±', 'ìŠ¹ì¸',
            'ì ì‘ì¦', 'í—ˆê°€', 'ê¸‰ì—¬'
        ]

        self.stats = GateChainStats()
        self.entries: List[DrugEntry] = []

        # ê²°ê³¼ ì»¨í…Œì´ë„ˆ
        self.active_drugs: List[DrugEntry] = []
        self.pending_drugs: List[DrugEntry] = []
        self.dropped_drugs: List[DrugEntry] = []
        self.regimens: List[DrugEntry] = []
        self.biomarkers: List[DrugEntry] = []
        self.diseases: List[DrugEntry] = []

        # ì¶©ëŒ ì¶”ì 
        self.ko_to_en_map: Dict[str, List[str]] = defaultdict(list)

    # =========================================================================
    # 1. ì •ê·œí™”
    # =========================================================================

    def normalize_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ê·œí™” (NFKC + ë”°ì˜´í‘œ/í•˜ì´í”ˆ/ê³µë°± í†µì¼)

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸

        Returns:
            ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""

        # Unicode ì •ê·œí™”
        text = unicodedata.normalize('NFKC', text)

        # ë”°ì˜´í‘œ í†µì¼
        for chars, target in self.filters['normalization']['quote_normalization']:
            for char in chars:
                text = text.replace(char, target)

        # í•˜ì´í”ˆ í†µì¼
        for chars, target in self.filters['normalization']['hyphen_normalization']:
            for char in chars:
                text = text.replace(char, target)

        # í”ŒëŸ¬ìŠ¤ í†µì¼
        for chars, target in self.filters['normalization']['plus_normalization']:
            for char in chars:
                text = text.replace(char, target)

        # ì¤‘ë³µ ê³µë°± ì œê±°
        if self.filters['normalization']['remove_duplicate_spaces']:
            text = re.sub(r'\s+', ' ', text)

        text = text.strip()

        return text

    def normalize_case(self, text: str, lang: str) -> str:
        """
        ëŒ€ì†Œë¬¸ì ì •ê·œí™”

        Args:
            text: í…ìŠ¤íŠ¸
            lang: ì–¸ì–´ ('en' ë˜ëŠ” 'ko')

        Returns:
            ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
        """
        handling = self.filters['normalization']['case_handling'].get(lang, 'as_is')

        if handling == 'lowercase':
            return text.lower()
        elif handling == 'uppercase':
            return text.upper()
        else:
            return text

    # =========================================================================
    # 2. ë¸Œëœë“œëª… í•´ì†Œ ë° ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
    # =========================================================================

    def resolve_brand_name(self, entry: DrugEntry) -> DrugEntry:
        """
        ë¸Œëœë“œëª…ì„ ì„±ë¶„ëª…ìœ¼ë¡œ í•´ì†Œ

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            í•´ì†Œëœ ì•½ì œ í•­ëª©
        """
        en_lower = entry.en.lower()
        ko = entry.ko

        # ì˜ë¬¸ ë¸Œëœë“œëª… ì²´í¬
        if en_lower in self.brand_to_ingredient:
            ingredient = self.brand_to_ingredient[en_lower]
            self.logger.info(f"Resolved brand: {entry.en} â†’ {ingredient}")
            entry.en = ingredient
            entry.reason_codes.append("BRAND_RESOLVED_EN")

        # í•œê¸€ ë¸Œëœë“œëª… ì²´í¬
        if ko in self.brand_to_ingredient:
            ingredient = self.brand_to_ingredient[ko]
            self.logger.info(f"Resolved brand: {entry.ko} â†’ {ingredient}")
            # í•œê¸€ì€ ë¸Œëœë“œëª… ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜, ì˜ë¬¸ë§Œ ì„±ë¶„ëª…ìœ¼ë¡œ ë³€ê²½
            if not entry.en or entry.en == ko:
                entry.en = ingredient
            entry.reason_codes.append("BRAND_RESOLVED_KO")

        return entry

    def has_context_signal(self, entry: DrugEntry) -> bool:
        """
        ì»¨í…ìŠ¤íŠ¸ì— ì„ìƒ ì‹œê·¸ë„ì´ ìˆëŠ”ì§€ í™•ì¸

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            ì»¨í…ìŠ¤íŠ¸ ì‹œê·¸ë„ ì¡´ì¬ ì—¬ë¶€
        """
        if not entry.context_span:
            return False

        # ì»¨í…ìŠ¤íŠ¸ í† í° ê²€ì‚¬
        for token in self.ctx_tokens:
            if token in entry.context_span:
                return True

        return False

    # =========================================================================
    # 3. ê²Œì´íŠ¸ 1 - ê¸ˆì¹™ì–´ í•„í„°
    # =========================================================================

    def check_forbidden_forms(self, entry: DrugEntry) -> Tuple[bool, List[str]]:
        """
        ê¸ˆì¹™ì–´(ì œí˜•/í¬ì¥ì–´) í•„í„°

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            (pass, reason_codes)
        """
        reasons = []

        ko = entry.ko

        # í•˜ë“œ ì»· (ë¬´ì¡°ê±´ ì œì™¸)
        hard_forms = self.filters['forbidden_forms']['hard']
        if ko in hard_forms:
            reasons.append("FORM_TERM")
            return False, reasons

        # ì¡°ê±´ë¶€ ì»· (mL, mg ë“±)
        conditional_forms = self.filters['forbidden_forms']['conditional']
        if ko in conditional_forms:
            # ì»¨í…ìŠ¤íŠ¸ í™•ì¸ (Â±20ì ì´ë‚´ì— ì„±ë¶„ ë‹¨ì„œê°€ ìˆëŠ”ì§€)
            if not self._has_ingredient_hint_in_context(entry):
                reasons.append("CONTEXT_PACKAGING")
                return False, reasons

        return True, reasons

    def _has_ingredient_hint_in_context(self, entry: DrugEntry) -> bool:
        """
        ì»¨í…ìŠ¤íŠ¸ ë‚´ì— ì„±ë¶„ ë‹¨ì„œê°€ ìˆëŠ”ì§€ í™•ì¸

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            ì„±ë¶„ ë‹¨ì„œ ì¡´ì¬ ì—¬ë¶€
        """
        # í˜„ì¬ëŠ” context_spanì´ ë¹„ì–´ìˆìœ¼ë¯€ë¡œ False ë°˜í™˜
        # ì‹¤ì œë¡œëŠ” ì›ë³¸ ë¬¸ì„œì—ì„œ Â±20ìë¥¼ ì¶”ì¶œí•´ì•¼ í•¨
        if not entry.context_span:
            return False

        ingredient_hints = self.filters['ingredient_hints']
        for hint in ingredient_hints:
            if hint in entry.context_span:
                return True

        return False

    # =========================================================================
    # 3. ê²Œì´íŠ¸ 2 - ì ‘ë¯¸ì‚¬ ì •í•©ì„±
    # =========================================================================

    def check_suffix_consistency(self, entry: DrugEntry) -> Tuple[bool, List[str], bool]:
        """
        ì ‘ë¯¸ì‚¬ ì •í•©ì„± ê²€ì‚¬ (EN â†” KO)

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            (pass, reason_codes, strict_suffix_matched)
        """
        reasons = []
        strict_suffix_matched = False

        en = entry.en.lower()
        ko = entry.ko

        # ì ‘ë¯¸ì‚¬ íŒíŠ¸ í™•ì¸
        for hint in self.filters['en_suffix_to_ko_hint']:
            en_suffix = hint['en']
            ko_suffixes = hint['ko'] if isinstance(hint['ko'], list) else [hint['ko']]
            strict = hint.get('strict', False)

            # EN ì ‘ë¯¸ì‚¬ ë§¤ì¹­
            if re.search(en_suffix + r'$', en):
                # KO ì ‘ë¯¸ì‚¬ ë§¤ì¹­ í™•ì¸
                ko_match = any(re.search(ko_suf + r'$', ko) for ko_suf in ko_suffixes)

                if not ko_match:
                    if strict:
                        # strict ëª¨ë“œì—ì„œëŠ” ë¶ˆì¼ì¹˜ ì‹œ ë³´ë¥˜
                        reasons.append("SUFFIX_MISMATCH")
                        return False, reasons, False
                    else:
                        # loose ëª¨ë“œì—ì„œëŠ” ê²½ê³ ë§Œ
                        reasons.append("SUFFIX_MISMATCH_WARN")
                else:
                    # ì ‘ë¯¸ì‚¬ ë§¤ì¹­ ì„±ê³µ
                    if strict:
                        strict_suffix_matched = True
                        reasons.append("SUFFIX_MATCH_STRICT")

        return True, reasons, strict_suffix_matched

    # =========================================================================
    # 4. ê²Œì´íŠ¸ 3 - ìŒì°¨/ì² ì ìœ ì‚¬ë„
    # =========================================================================

    def check_phonetic_similarity(self, entry: DrugEntry) -> Tuple[bool, List[str]]:
        """
        ìŒì°¨/ì² ì ìœ ì‚¬ë„ ê²€ì‚¬

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            (pass, reason_codes)
        """
        reasons = []

        en = entry.en.lower()
        ko = entry.ko

        # í•œê¸€ â†’ ë¡œë§ˆì ë³€í™˜ (ê°„ë‹¨í•œ ìŒì°¨)
        ko_romanized = self._romanize_korean(ko)

        # í¸ì§‘ ê±°ë¦¬ ê³„ì‚°
        similarity = SequenceMatcher(None, en, ko_romanized).ratio()

        # ì„ê³„ê°’ ì„ íƒ (ê³ ë¹ˆë„ vs í¬ê·€)
        threshold = (
            self.filters['phonetic_threshold']['strict']
            if entry.count >= self.filters['high_frequency_threshold']
            else self.filters['phonetic_threshold']['loose']
        )

        # ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ë³´ë¥˜
        if 1 - similarity > threshold:  # distance = 1 - similarity
            reasons.append("PHONETIC_FAIL")
            return False, reasons

        return True, reasons

    def _romanize_korean(self, text: str) -> str:
        """
        í•œê¸€ â†’ ë¡œë§ˆì ë³€í™˜ (ê°„ë‹¨í•œ ìŒì°¨)

        Args:
            text: í•œê¸€ í…ìŠ¤íŠ¸

        Returns:
            ë¡œë§ˆì í…ìŠ¤íŠ¸
        """
        # ê°„ë‹¨í•œ ìëª¨ ë¶„í•´ ë° ë¡œë§ˆì ë³€í™˜
        # ì‹¤ì œë¡œëŠ” jamo, romanize ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ëœ ë²„ì „

        # ì´ˆì„±, ì¤‘ì„±, ì¢…ì„± ìœ ë‹ˆì½”ë“œ ì˜¤í”„ì…‹
        CHOSUNG_BASE = 0x1100
        JUNGSUNG_BASE = 0x1161
        JONGSUNG_BASE = 0x11A8
        HANGUL_BASE = 0xAC00

        CHOSUNG_LIST = ['g', 'kk', 'n', 'd', 'tt', 'r', 'm', 'b', 'pp', 's', 'ss', '', 'j', 'jj', 'ch', 'k', 't', 'p', 'h']
        JUNGSUNG_LIST = ['a', 'ae', 'ya', 'yae', 'eo', 'e', 'yeo', 'ye', 'o', 'wa', 'wae', 'oe', 'yo', 'u', 'weo', 'we', 'wi', 'yu', 'eu', 'ui', 'i']
        JONGSUNG_LIST = ['', 'g', 'kk', 'gs', 'n', 'nj', 'nh', 'd', 'l', 'lg', 'lm', 'lb', 'ls', 'lt', 'lp', 'lh', 'm', 'b', 'bs', 's', 'ss', 'ng', 'j', 'ch', 'k', 't', 'p', 'h']

        result = []
        for char in text:
            code = ord(char)
            if 0xAC00 <= code <= 0xD7A3:  # í•œê¸€ ìŒì ˆ
                code -= HANGUL_BASE
                cho = code // (21 * 28)
                jung = (code % (21 * 28)) // 28
                jong = code % 28

                result.append(CHOSUNG_LIST[cho])
                result.append(JUNGSUNG_LIST[jung])
                if jong != 0:
                    result.append(JONGSUNG_LIST[jong])
            else:
                result.append(char)

        return ''.join(result)

    def is_curated_pair(self, entry: DrugEntry) -> bool:
        """
        íë ˆì´ì…˜ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ìŒì¸ì§€ í™•ì¸

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            íë ˆì´ì…˜ ì—¬ë¶€
        """
        pair_key = (entry.en.lower(), entry.ko)
        return pair_key in self.curated_pairs

    # =========================================================================
    # 5. ê²Œì´íŠ¸ 4 - ATC/ê³„ì—´ êµì°¨ê²€ì¦ (ì„ íƒ)
    # =========================================================================

    def check_atc_consistency(self, entry: DrugEntry) -> Tuple[bool, List[str]]:
        """
        ATC ê³„ì—´ êµì°¨ê²€ì¦ (í˜„ì¬ëŠ” ìŠ¤í‚µ)

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            (pass, reason_codes)
        """
        # ATC ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ìŠ¤í‚µ
        return True, []

    # =========================================================================
    # 6. ê²Œì´íŠ¸ 5 - ë ˆì§/ë°”ì´ì˜¤ë§ˆì»¤/ì§ˆí™˜ì¶• ë¶„ë¦¬
    # =========================================================================

    def check_routing(self, entry: DrugEntry) -> Tuple[bool, List[str], Optional[str]]:
        """
        ë ˆì§/ë°”ì´ì˜¤ë§ˆì»¤/ì§ˆí™˜ì¶• ë¶„ë¦¬

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            (is_routed, reason_codes, route_target)
        """
        reasons = []

        en = entry.en.upper()
        ko = entry.ko

        # ë ˆì§ íŒ¨í„´ í™•ì¸
        for pattern in self.filters['patterns']['regimen']:
            if re.search(pattern, en) or re.search(pattern, ko):
                reasons.append("ROUTE_REGIMEN")
                return True, reasons, "regimen"

        # ë°”ì´ì˜¤ë§ˆì»¤ íŒ¨í„´ í™•ì¸
        for pattern in self.filters['patterns']['biomarker']:
            if re.search(pattern, en, re.IGNORECASE) or re.search(pattern, ko):
                reasons.append("ROUTE_BIOMARKER")
                return True, reasons, "biomarker"

        # ì§ˆí™˜ íŒ¨í„´ í™•ì¸
        for pattern in self.filters['patterns']['disease']:
            if re.search(pattern, en, re.IGNORECASE) or re.search(pattern, ko):
                reasons.append("ROUTE_DISEASE")
                return True, reasons, "disease"

        return False, reasons, None

    # =========================================================================
    # 7. ê²Œì´íŠ¸ 6 - ì¶©ëŒ í•´ì†Œ
    # =========================================================================

    def check_conflicts(self, entry: DrugEntry) -> Tuple[bool, List[str]]:
        """
        ì¶©ëŒ í•´ì†Œ (ë™ì¼ ko â†” ìƒì´ en)

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            (pass, reason_codes)
        """
        reasons = []

        ko = entry.ko
        en = entry.en

        # í˜„ì¬ koì— ëŒ€í•œ en ëª©ë¡ í™•ì¸
        if ko in self.ko_to_en_map:
            existing_ens = self.ko_to_en_map[ko]
            if en not in existing_ens:
                # ì¶©ëŒ ë°œìƒ
                reasons.append("ALIAS_CONFLICT")
                self.logger.warning(f"Conflict detected: {ko} â†” {existing_ens} vs {en}")
                return False, reasons

        # ì¶©ëŒ ì—†ìœ¼ë©´ ë“±ë¡
        self.ko_to_en_map[ko].append(en)

        return True, reasons

    # =========================================================================
    # 8. ê²Œì´íŠ¸ ì²´ì¸ ì‹¤í–‰
    # =========================================================================

    def apply_gate_chain(self, entry: DrugEntry) -> DrugEntry:
        """
        ê²Œì´íŠ¸ ì²´ì¸ ì ìš©

        Args:
            entry: ì•½ì œ í•­ëª©

        Returns:
            ì²˜ë¦¬ëœ ì•½ì œ í•­ëª©
        """
        all_reasons = []

        # ìµœìš°ì„ : íë ˆì´ì…˜ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì²´í¬ (ëª¨ë“  ê²Œì´íŠ¸ ìš°íšŒ)
        if self.is_curated_pair(entry):
            self.logger.info(f"Curated pair - bypassing all gates: {entry.en} â†’ {entry.ko}")
            entry.decision = "active"
            entry.reason_codes = ["PASS_ALL", "CURATED_WHITELIST"]
            return entry

        # ì „ì²˜ë¦¬: ë¸Œëœë“œëª… í•´ì†Œ
        entry = self.resolve_brand_name(entry)

        # ê²Œì´íŠ¸ 1: ê¸ˆì¹™ì–´ í•„í„°
        pass_gate1, reasons1 = self.check_forbidden_forms(entry)
        all_reasons.extend(reasons1)
        if not pass_gate1:
            entry.decision = "drop"
            entry.reason_codes = all_reasons
            return entry

        # ê²Œì´íŠ¸ 2: ë¼ìš°íŒ… (ë ˆì§/ë°”ì´ì˜¤ë§ˆì»¤/ì§ˆí™˜ì€ ì¡°ê¸° ë¶„ë¥˜)
        is_routed, reasons2, route_target = self.check_routing(entry)
        all_reasons.extend(reasons2)
        if is_routed:
            entry.decision = f"route_{route_target}"
            entry.reason_codes = all_reasons
            return entry

        # ê²Œì´íŠ¸ 3: ì ‘ë¯¸ì‚¬ ì •í•©ì„±
        pass_gate3, reasons3, strict_suffix_matched = self.check_suffix_consistency(entry)
        all_reasons.extend(reasons3)
        if not pass_gate3:
            entry.decision = "pending"
            entry.reason_codes = all_reasons
            return entry

        # ê²Œì´íŠ¸ 4: ìŒì°¨/ì² ì ìœ ì‚¬ë„
        # strict suffixê°€ ë§¤ì¹­ëœ ê²½ìš°, ìŒì°¨ ê²€ì‚¬ ìŠ¤í‚µ
        if strict_suffix_matched:
            phonetic_passed = True
        else:
            pass_gate4, reasons4 = self.check_phonetic_similarity(entry)
            all_reasons.extend(reasons4)
            if not pass_gate4:
                phonetic_passed = False
                # ì»¨í…ìŠ¤íŠ¸ ìŠ¹ê²© ë¡œì§: ìŒì°¨ ì‹¤íŒ¨ + ì»¨í…ìŠ¤íŠ¸ ì‹œê·¸ë„
                if self.has_context_signal(entry):
                    self.logger.info(f"Context promotion: {entry.en} â†’ {entry.ko}")
                    all_reasons.append("CTX_PROMOTE")
                    phonetic_passed = True  # ìŠ¹ê²©
                else:
                    entry.decision = "pending"
                    entry.reason_codes = all_reasons
                    return entry

        # ê²Œì´íŠ¸ 5: ATC êµì°¨ê²€ì¦ (ìŠ¤í‚µ)
        pass_gate5, reasons5 = self.check_atc_consistency(entry)
        all_reasons.extend(reasons5)
        if not pass_gate5:
            entry.decision = "pending"
            entry.reason_codes = all_reasons
            return entry

        # ê²Œì´íŠ¸ 6: ì¶©ëŒ í•´ì†Œ
        pass_gate6, reasons6 = self.check_conflicts(entry)
        all_reasons.extend(reasons6)
        if not pass_gate6:
            entry.decision = "pending"
            entry.reason_codes = all_reasons
            return entry

        # ëª¨ë“  ê²Œì´íŠ¸ í†µê³¼
        entry.decision = "active"
        entry.reason_codes = ["PASS_ALL"] + all_reasons

        return entry

    # =========================================================================
    # 9. ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    # =========================================================================

    def load_input(self, input_path: str, dry_run_limit: Optional[int] = None) -> List[DrugEntry]:
        """
        ì…ë ¥ íŒŒì¼ ë¡œë“œ (JSON ë˜ëŠ” CSV)

        Args:
            input_path: ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            dry_run_limit: ë“œë¼ì´ëŸ° ì‹œ ì²˜ë¦¬í•  í•­ëª© ìˆ˜

        Returns:
            ì•½ì œ í•­ëª© ë¦¬ìŠ¤íŠ¸
        """
        import csv

        entries = []
        input_file = Path(input_path)

        if input_file.suffix == '.csv':
            # CSV ë¡œë“œ: contextì—ì„œ ê´„í˜¸ìŒì„ ì¶”ì¶œí•˜ì—¬ en-ko ë§¤í•‘ ìƒì„±
            pair_pattern = r'([ê°€-í£][ê°€-í£\s]+)\s*\(([A-Za-z][A-Za-z\s\-]+)\)|([A-Za-z][A-Za-z\s\-]+)\s*\(([ê°€-í£][ê°€-í£\s]+)\)'

            with open(input_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                seen_pairs = set()  # ì¤‘ë³µ ì œê±°

                for row in reader:
                    surface = row.get('surface', '')
                    lang = row.get('lang', '')
                    context = row.get('context', '')

                    # contextì—ì„œ ê´„í˜¸ìŒ ì°¾ê¸°
                    for match in re.finditer(pair_pattern, context):
                        if match.group(1) and match.group(2):
                            ko = match.group(1).strip()
                            en = match.group(2).strip()
                        elif match.group(3) and match.group(4):
                            en = match.group(3).strip()
                            ko = match.group(4).strip()
                        else:
                            continue

                        # ì¤‘ë³µ ì œê±°
                        pair_key = (en.lower(), ko)
                        if pair_key in seen_pairs:
                            continue
                        seen_pairs.add(pair_key)

                        # ì •ê·œí™”
                        en_norm = self.normalize_case(self.normalize_text(en), 'en')
                        ko_norm = self.normalize_case(self.normalize_text(ko), 'ko')

                        entry = DrugEntry(
                            en=en_norm,
                            ko=ko_norm,
                            count=1,
                            source=row.get('src', 'csv'),
                            context_span=context[:200]  # ì»¨í…ìŠ¤íŠ¸ 200ìë¡œ ì œí•œ
                        )
                        entries.append(entry)

        else:
            # JSON ë¡œë“œ
            with open(input_path, 'r', encoding='utf-8-sig') as f:
                data = json.load(f)

            # matched_via_english ì„¹ì…˜ ì²˜ë¦¬
            if 'matched_via_english' in data:
                for item in data['matched_via_english']:
                    entry = DrugEntry(
                        en=self.normalize_case(self.normalize_text(item.get('english', '')), 'en'),
                        ko=self.normalize_case(self.normalize_text(item.get('korean', '')), 'ko'),
                        count=item.get('count', 0),
                        source='matched_via_english'
                    )
                    entries.append(entry)

        # ë“œë¼ì´ëŸ° ì œí•œ
        if dry_run_limit:
            entries = entries[:dry_run_limit]

        self.stats.total_input = len(entries)
        self.logger.info(f"Loaded {len(entries)} entries")

        return entries

    def _build_curated_pairs_set(self) -> None:
        """íë ˆì´ì…˜ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì„¸íŠ¸ êµ¬ì¶• (ì •ê·œí™” ì ìš©)"""
        self.curated_pairs = set()
        for pair in self.curated_pairs_raw:
            en_norm = self.normalize_case(self.normalize_text(pair['en']), 'en')
            ko_norm = self.normalize_case(self.normalize_text(pair['ko']), 'ko')
            self.curated_pairs.add((en_norm, ko_norm))
        self.logger.info(f"Built {len(self.curated_pairs)} normalized curated pairs")

    def process_all(self, entries: List[DrugEntry]) -> None:
        """
        ëª¨ë“  í•­ëª© ì²˜ë¦¬

        Args:
            entries: ì•½ì œ í•­ëª© ë¦¬ìŠ¤íŠ¸
        """
        # íë ˆì´ì…˜ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì„¸íŠ¸ êµ¬ì¶• (ì •ê·œí™” ì ìš©)
        self._build_curated_pairs_set()

        progress_interval = self.filters['execution']['progress_interval']

        for idx, entry in enumerate(entries, 1):
            # ê²Œì´íŠ¸ ì²´ì¸ ì ìš©
            processed = self.apply_gate_chain(entry)

            # ê²°ê³¼ ë¶„ë¥˜
            if processed.decision == "active":
                self.active_drugs.append(processed)
                self.stats.active += 1
            elif processed.decision == "pending":
                self.pending_drugs.append(processed)
                self.stats.pending += 1
            elif processed.decision == "drop":
                self.dropped_drugs.append(processed)
                self.stats.dropped += 1
            elif processed.decision.startswith("route_"):
                route_target = processed.decision.split('_')[1]
                if route_target == "regimen":
                    self.regimens.append(processed)
                    self.stats.routed_regimen += 1
                elif route_target == "biomarker":
                    self.biomarkers.append(processed)
                    self.stats.routed_biomarker += 1
                elif route_target == "disease":
                    self.diseases.append(processed)
                    self.stats.routed_disease += 1

            # reason_codes í†µê³„
            for code in processed.reason_codes:
                self.stats.reason_code_counts[code] += 1

            # ì§„í–‰ ìƒí™© ë¡œê·¸
            if idx % progress_interval == 0:
                self.logger.info(f"Processed {idx}/{len(entries)} entries")

        self.logger.info(f"Processing complete: {len(entries)} entries")

    # =========================================================================
    # 10. ì¶œë ¥
    # =========================================================================

    def save_yaml(self, entries: List[DrugEntry], output_path: str, section: str = "active") -> None:
        """
        YAML íŒŒì¼ ì €ì¥

        Args:
            entries: ì•½ì œ í•­ëª© ë¦¬ìŠ¤íŠ¸
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            section: ì„¹ì…˜ëª… (active, pending ë“±)
        """
        output_data = {
            section: [
                {
                    'canonical_en': entry.en,
                    'canonical_ko': entry.ko,
                    'aliases': entry.aliases,
                    'reason_codes': entry.reason_codes,
                    'count': entry.count,
                    'source': entry.source
                }
                for entry in entries
            ]
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(output_data, f, allow_unicode=True, sort_keys=False)

        self.logger.info(f"Saved {len(entries)} entries to {output_path}")

    def save_jsonl_log(self, log_path: str) -> None:
        """
        JSONL ë¡œê·¸ ì €ì¥

        Args:
            log_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        """
        all_entries = (
            self.active_drugs + self.pending_drugs + self.dropped_drugs +
            self.regimens + self.biomarkers + self.diseases
        )

        with open(log_path, 'w', encoding='utf-8') as f:
            for entry in all_entries:
                log_entry = {
                    'en': entry.en,
                    'ko': entry.ko,
                    'decision': entry.decision,
                    'reason_codes': entry.reason_codes,
                    'count': entry.count,
                    'source': entry.source
                }
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

        self.logger.info(f"Saved log to {log_path}")

    def generate_report(self, report_path: str) -> None:
        """
        Markdown ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            report_path: ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ
        """
        lines = []

        lines.append("# ê²Œì´íŠ¸ ì²´ì¸ ì•½ì œ ë§¤ì¹­ ì •ì œ ë¦¬í¬íŠ¸\n")
        lines.append(f"**ìƒì„±ì¼**: {Path().cwd()}\n")
        lines.append("---\n\n")

        # í†µê³„ ìš”ì•½
        lines.append("## ğŸ“Š í†µê³„ ìš”ì•½\n")
        lines.append(f"- **ì´ ì…ë ¥**: {self.stats.total_input}ê±´\n")
        lines.append(f"- **ê²°ì • (active)**: {self.stats.active}ê±´ ({self.stats.active/self.stats.total_input*100:.1f}%)\n")
        lines.append(f"- **ë³´ë¥˜ (pending)**: {self.stats.pending}ê±´ ({self.stats.pending/self.stats.total_input*100:.1f}%)\n")
        lines.append(f"- **ì œì™¸ (dropped)**: {self.stats.dropped}ê±´ ({self.stats.dropped/self.stats.total_input*100:.1f}%)\n")
        lines.append(f"- **ë¼ìš°íŒ…**: {self.stats.routed_regimen + self.stats.routed_biomarker + self.stats.routed_disease}ê±´\n")
        lines.append(f"  - ë ˆì§: {self.stats.routed_regimen}ê±´\n")
        lines.append(f"  - ë°”ì´ì˜¤ë§ˆì»¤: {self.stats.routed_biomarker}ê±´\n")
        lines.append(f"  - ì§ˆí™˜: {self.stats.routed_disease}ê±´\n\n")

        # Reason codes Top 10
        lines.append("## ğŸ† Reason Codes Top 10\n")
        for code, count in self.stats.reason_code_counts.most_common(10):
            lines.append(f"- **{code}**: {count}ê±´\n")
        lines.append("\n")

        # ìƒ˜í”Œ ì¼€ì´ìŠ¤
        lines.append("## ğŸ“‹ ìƒ˜í”Œ ì¼€ì´ìŠ¤\n")

        # ì œì™¸ ì˜ˆì‹œ
        lines.append("### ì œì™¸ (Dropped)\n")
        for entry in self.dropped_drugs[:3]:
            lines.append(f"- `{entry.en} â†’ {entry.ko}` ({', '.join(entry.reason_codes)})\n")
        lines.append("\n")

        # ë³´ë¥˜ ì˜ˆì‹œ
        lines.append("### ë³´ë¥˜ (Pending)\n")
        for entry in self.pending_drugs[:3]:
            lines.append(f"- `{entry.en} â†’ {entry.ko}` ({', '.join(entry.reason_codes)})\n")
        lines.append("\n")

        # í™œì„± ì˜ˆì‹œ
        lines.append("### í™œì„± (Active)\n")
        for entry in self.active_drugs[:5]:
            lines.append(f"- `{entry.en} â†’ {entry.ko}` (count: {entry.count})\n")
        lines.append("\n")

        # ìˆ˜ë½ ê¸°ì¤€ ê²€ì¦
        lines.append("## âœ… ìˆ˜ë½ ê¸°ì¤€ ê²€ì¦\n")

        # ì œí˜•/í¬ì¥ì–´ í™•ì¸
        forms_in_active = []
        hard_forms = self.filters['forbidden_forms']['hard']
        for entry in self.active_drugs:
            if entry.ko in hard_forms:
                forms_in_active.append(entry)

        if forms_in_active:
            lines.append(f"- âŒ **ì œí˜•/í¬ì¥ì–´ ë°œê²¬**: {len(forms_in_active)}ê±´\n")
            for entry in forms_in_active:
                lines.append(f"  - `{entry.en} â†’ {entry.ko}`\n")
        else:
            lines.append("- âœ… **ì œí˜•/í¬ì¥ì–´ 0ê±´** (í†µê³¼)\n")

        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ê²€ì¦
        lines.append("\n### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ê²€ì¦\n")
        test_cases = self.filters['acceptance_criteria']['test_cases']

        for tc in test_cases:
            en_test = tc['input']['en']
            ko_test = tc['input']['ko']
            expected = tc['expected']

            # í•´ë‹¹ í•­ëª© ì°¾ê¸°
            found = None
            for entry in (self.active_drugs + self.pending_drugs + self.dropped_drugs +
                         self.regimens + self.biomarkers + self.diseases):
                if entry.en == en_test and entry.ko == ko_test:
                    found = entry
                    break

            if found:
                actual = found.decision
                if (expected == "active" and actual == "active") or \
                   (expected == "pending" and actual == "pending") or \
                   (expected == "drop" and actual == "drop"):
                    lines.append(f"- âœ… `{en_test} â†’ {ko_test}`: {expected} (í†µê³¼)\n")
                else:
                    lines.append(f"- âŒ `{en_test} â†’ {ko_test}`: ì˜ˆìƒ {expected}, ì‹¤ì œ {actual} (ì‹¤íŒ¨)\n")
            else:
                lines.append(f"- âš ï¸ `{en_test} â†’ {ko_test}`: í•­ëª© ì—†ìŒ\n")

        # íŒŒì¼ ì €ì¥
        with open(report_path, 'w', encoding='utf-8') as f:
            f.writelines(lines)

        self.logger.info(f"Report generated: {report_path}")

# =============================================================================
# ë©”ì¸ í•¨ìˆ˜
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description='ê²Œì´íŠ¸ ì²´ì¸ ê¸°ë°˜ ì•½ì œ ë§¤ì¹­ ì •ì œ')
    parser.add_argument('--input', required=True, help='ì…ë ¥ JSON íŒŒì¼')
    parser.add_argument('--filters', required=True, help='filters.yaml íŒŒì¼')
    parser.add_argument('--brand-alias', help='brand_alias.yaml íŒŒì¼ (ì„ íƒ)')
    parser.add_argument('--curated-pairs', help='íë ˆì´ì…˜ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ JSON íŒŒì¼ (ì„ íƒ)')
    parser.add_argument('--out-drug', required=True, help='ì¶œë ¥ drug.yaml íŒŒì¼')
    parser.add_argument('--log', required=True, help='ë¡œê·¸ JSONL íŒŒì¼')
    parser.add_argument('--report', required=True, help='ë¦¬í¬íŠ¸ MD íŒŒì¼')
    parser.add_argument('--dry-run', action='store_true', help='ë“œë¼ì´ëŸ° ëª¨ë“œ (200ê±´ë§Œ ì²˜ë¦¬)')

    args = parser.parse_args()

    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # ì •ì œê¸° ì´ˆê¸°í™”
    refiner = DrugAnchorRefiner(args.filters, args.brand_alias, args.curated_pairs)

    # ì…ë ¥ ë¡œë“œ
    dry_run_limit = 200 if args.dry_run else None
    entries = refiner.load_input(args.input, dry_run_limit)

    # ì²˜ë¦¬
    refiner.process_all(entries)

    # ì¶œë ¥
    # drug.yaml (active + pending)
    output_data = {
        'active': [
            {
                'canonical_en': entry.en,
                'canonical_ko': entry.ko,
                'aliases': entry.aliases,
                'reason_codes': entry.reason_codes,
                'count': entry.count
            }
            for entry in refiner.active_drugs
        ],
        'pending': [
            {
                'canonical_en': entry.en,
                'canonical_ko': entry.ko,
                'aliases': entry.aliases,
                'reason_codes': entry.reason_codes,
                'count': entry.count
            }
            for entry in refiner.pending_drugs
        ]
    }

    with open(args.out_drug, 'w', encoding='utf-8') as f:
        yaml.dump(output_data, f, allow_unicode=True, sort_keys=False)

    # ë¼ìš°íŒ… íŒŒì¼ë“¤
    if refiner.regimens:
        refiner.save_yaml(refiner.regimens, 'dictionary/anchor/regimen.yaml', 'regimen')
    if refiner.biomarkers:
        refiner.save_yaml(refiner.biomarkers, 'dictionary/anchor/biomarker.yaml', 'biomarker')
    if refiner.diseases:
        refiner.save_yaml(refiner.diseases, 'dictionary/anchor/disease_alias.yaml', 'disease')

    # JSONL ë¡œê·¸
    refiner.save_jsonl_log(args.log)

    # ë¦¬í¬íŠ¸
    refiner.generate_report(args.report)

    print(f"\n[SUCCESS] Processing complete!")
    print(f"   - Active: {refiner.stats.active}")
    print(f"   - Pending: {refiner.stats.pending}")
    print(f"   - Dropped: {refiner.stats.dropped}")
    print(f"   - Routed: {refiner.stats.routed_regimen + refiner.stats.routed_biomarker + refiner.stats.routed_disease}")

if __name__ == '__main__':
    main()
