"""
ì§ˆë³‘ì½”ë”©ì‚¬ë¡€ì§‘ PDF ì§€ëŠ¥í˜• ë¶„í•  íŒŒì‹±
- ê· ë“±í•˜ê²Œ 50í˜ì´ì§€ì”© ë¶„í• 
- Upstage APIë¡œ ê° ì²­í¬ íŒŒì‹±
"""
import sys
import codecs
from pathlib import Path
import json
import os
from datetime import datetime
from dotenv import load_dotenv
from pypdf import PdfReader, PdfWriter
import requests
import time

# UTF-8 ì¶œë ¥
if sys.platform == 'win32':
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()


class SmartPDFSplitParser:
    """ê· ë“± ë¶„í•  PDF íŒŒì„œ"""

    def __init__(self, output_dir: str = 'data/hira_master/parsed_smart', target_chunk_pages: int = 50):
        """
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            target_chunk_pages: ëª©í‘œ ì²­í¬ í¬ê¸° (í˜ì´ì§€ ìˆ˜)
        """
        self.api_key = os.getenv('UPSTAGE_API_KEY')
        if not self.api_key:
            raise ValueError('UPSTAGE_API_KEY not found in .env')

        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.target_chunk_pages = target_chunk_pages
        self.base_url = "https://api.upstage.ai/v1/document-ai/document-parse"

        # ì„ì‹œ ë¶„í•  íŒŒì¼ ì €ì¥ ìœ„ì¹˜
        self.temp_dir = self.output_dir / 'temp_chunks'
        self.temp_dir.mkdir(exist_ok=True)

    def analyze_structure(self, pdf_path: Path) -> dict:
        """PDF êµ¬ì¡° ë¶„ì„ - ê· ë“± ë¶„í• """
        reader = PdfReader(pdf_path)
        total_pages = len(reader.pages)

        # ê· ë“±í•˜ê²Œ target_chunk_pagesì”© ë¶„í• 
        major_sections = []
        current_page = 1
        chunk_num = 1

        while current_page <= total_pages:
            end_page = min(current_page + self.target_chunk_pages - 1, total_pages)
            major_sections.append({
                'name': f'ì„¹ì…˜ {chunk_num} (p.{current_page}-{end_page})',
                'start': current_page,
                'end': end_page
            })
            current_page = end_page + 1
            chunk_num += 1

        return {
            'total_pages': total_pages,
            'major_sections': major_sections
        }

    def calculate_smart_splits(self, structure: dict) -> list:
        """ë¶„í•  ì§€ì  ê³„ì‚° - major_sectionsë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©"""
        major_sections = structure['major_sections']
        splits = []

        for section in major_sections:
            splits.append({
                'start': section['start'],
                'end': section['end'],
                'pages': section['end'] - section['start'] + 1,
                'sections': [section['name']]
            })

        return splits

    def split_pdf(self, pdf_path: Path, splits: list) -> list:
        """ê³„ì‚°ëœ ë¶„í•  ì§€ì ì— ë”°ë¼ PDF ë¶„í• """
        print(f'\nğŸ“„ ì§€ëŠ¥í˜• PDF ë¶„í• : {pdf_path.name}')

        reader = PdfReader(pdf_path)
        total_pages = len(reader.pages)

        print(f'  ì´ í˜ì´ì§€: {total_pages}p')
        print(f'  ë¶„í•  ê³„íš: {len(splits)}ê°œ ì²­í¬')

        for i, split in enumerate(splits, 1):
            print(f'    ì²­í¬{i}: p.{split["start"]}-{split["end"]} ({split["pages"]}p)')

        chunks = []

        for i, split in enumerate(splits, 1):
            start_page = split['start'] - 1  # 0-indexed
            end_page = split['end']

            # ìƒˆ PDF ìƒì„±
            writer = PdfWriter()
            for page_num in range(start_page, end_page):
                writer.add_page(reader.pages[page_num])

            # ì €ì¥
            chunk_filename = f'{pdf_path.stem}_smart_chunk{i:02d}_p{split["start"]:04d}-{split["end"]:04d}.pdf'
            chunk_path = self.temp_dir / chunk_filename
            writer.write(chunk_path)

            chunks.append({
                'path': chunk_path,
                'start': split['start'],
                'end': split['end'],
                'pages': split['pages'],
                'sections': split['sections']
            })
            print(f'  âœ… {chunk_filename}')

        return chunks

    def parse_chunk(self, chunk: dict, output_format: str = "html") -> dict:
        """ë‹¨ì¼ ì²­í¬ íŒŒì‹±"""
        chunk_path = chunk['path']
        print(f'\n[íŒŒì‹±] {chunk_path.name}')

        with open(chunk_path, 'rb') as f:
            files = {'document': (chunk_path.name, f, 'application/pdf')}
            data = {
                'ocr': 'true',
                'output_formats': f'["{output_format}"]',
            }

            start_time = time.time()
            try:
                response = requests.post(
                    self.base_url,
                    headers={"Authorization": f"Bearer {self.api_key}"},
                    files=files,
                    data=data,
                    timeout=300
                )
                elapsed = time.time() - start_time

                if response.status_code != 200:
                    error_msg = f"API Error {response.status_code}: {response.text}"
                    print(f'  âŒ ì‹¤íŒ¨: {error_msg}')
                    return {'error': error_msg, 'chunk': chunk}

                result = response.json()

                # content ì¶”ì¶œ
                content_dict = result.get('content', {})
                if isinstance(content_dict, dict):
                    content_text = content_dict.get(output_format, '')
                else:
                    content_text = str(content_dict)

                result['chunk_metadata'] = {
                    'chunk_info': {
                        'start_page': chunk['start'],
                        'end_page': chunk['end'],
                        'pages': chunk['pages'],
                        'sections': chunk['sections']
                    },
                    'chunk_file': str(chunk_path),
                    'elapsed_seconds': elapsed,
                    'content_length': len(content_text),
                    'api_pages': result.get('usage', {}).get('pages', 0)
                }

                print(f'  âœ… ì„±ê³µ: {elapsed:.1f}ì´ˆ, {len(content_text):,}ì')
                return result

            except Exception as e:
                print(f'  âŒ ì—ëŸ¬: {e}')
                return {'error': str(e), 'chunk': chunk}

    def parse_chunks(self, chunks: list) -> list:
        """ë¶„í• ëœ PDF íŒŒì¼ë“¤ íŒŒì‹±"""
        print(f'\nğŸ”„ ì²­í¬ íŒŒì‹±: {len(chunks)}ê°œ')

        results = []

        for i, chunk in enumerate(chunks, 1):
            print(f'\n[{i}/{len(chunks)}]')
            result = self.parse_chunk(chunk)
            results.append(result)

            # Rate limit ë°©ì§€
            if i < len(chunks):
                time.sleep(1)

        return results

    def merge_results(self, results: list, source_file: str, output_format: str = 'html') -> dict:
        """íŒŒì‹± ê²°ê³¼ ë³‘í•©"""
        print(f'\nğŸ”— ê²°ê³¼ ë³‘í•©')

        # ì—ëŸ¬ê°€ ì—†ëŠ” ê²°ê³¼ë§Œ í•„í„°ë§
        valid_results = [r for r in results if 'error' not in r]

        if not valid_results:
            print('  âŒ ëª¨ë“  ì²­í¬ íŒŒì‹± ì‹¤íŒ¨')
            return None

        # Content ë³‘í•©
        merged_content = []
        for r in valid_results:
            content_dict = r.get('content', {})
            if isinstance(content_dict, dict):
                content_text = content_dict.get(output_format, '')
            else:
                content_text = str(content_dict)
            merged_content.append(content_text)

        merged_content_str = '\n\n<hr>\n\n'.join(merged_content)

        # Elements ë³‘í•©
        merged_elements = []
        for r in valid_results:
            if 'elements' in r:
                merged_elements.extend(r['elements'])

        # ì´ í˜ì´ì§€ ìˆ˜
        total_pages = sum(r.get('chunk_metadata', {}).get('api_pages', 0) for r in valid_results)

        # ì²­í¬ ë©”íƒ€ë°ì´í„°
        chunks_metadata = []
        for r in valid_results:
            if 'chunk_metadata' in r:
                chunks_metadata.append(r['chunk_metadata'])

        merged = {
            'source_file': source_file,
            'total_pages': total_pages,
            'chunks_parsed': len(valid_results),
            'chunks_failed': len(results) - len(valid_results),
            'content': merged_content_str,
            'elements': merged_elements,
            'output_format': output_format,
            'parsing_method': 'uniform_split',
            'parsed_at': datetime.now().isoformat(),
            'chunks_metadata': chunks_metadata
        }

        print(f'  âœ… ë³‘í•© ì™„ë£Œ: {total_pages}p, {len(merged_content_str):,}ì')
        print(f'  âœ… Elements: {len(merged_elements)}ê°œ')

        # ì„¹ì…˜ë³„ ìš”ì•½
        print(f'\n  ğŸ“‹ ì„¹ì…˜ë³„ ë¶„í• :')
        for i, meta in enumerate(chunks_metadata, 1):
            chunk_info = meta['chunk_info']
            print(f'     ì²­í¬{i}: p.{chunk_info["start_page"]}-{chunk_info["end_page"]} '
                  f'({chunk_info["pages"]}p)')

        return merged

    def cleanup_temp_files(self):
        """ì„ì‹œ íŒŒì¼ ì‚­ì œ"""
        print(f'\nğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬')

        for file in self.temp_dir.glob('*.pdf'):
            file.unlink()
            print(f'  ì‚­ì œ: {file.name}')

    def parse_large_pdf(self, pdf_path: Path, output_format: str = 'html') -> dict:
        """ëŒ€ìš©ëŸ‰ PDF ì „ì²´ íŒŒì‹± í”„ë¡œì„¸ìŠ¤ (ì§€ëŠ¥í˜• ë¶„í• )"""
        print(f'\n{"="*80}')
        print(f'ğŸ“š ì§€ëŠ¥í˜• PDF íŒŒì‹±: {pdf_path.name}')
        print(f'{"="*80}')

        # 1. êµ¬ì¡° ë¶„ì„
        print('\n[ë‹¨ê³„ 1] PDF êµ¬ì¡° ë¶„ì„')
        structure = self.analyze_structure(pdf_path)
        print(f'  ì´ í˜ì´ì§€: {structure["total_pages"]}p')
        print(f'  ë¶„í•  ê³„íš: {len(structure["major_sections"])}ê°œ ì²­í¬')

        # 2. ë¶„í•  ê³„íš ìˆ˜ë¦½
        print('\n[ë‹¨ê³„ 2] ì§€ëŠ¥í˜• ë¶„í•  ê³„íš ìˆ˜ë¦½')
        splits = self.calculate_smart_splits(structure)

        # 3. PDF ë¶„í• 
        print('\n[ë‹¨ê³„ 3] PDF ë¶„í• ')
        chunks = self.split_pdf(pdf_path, splits)

        # 4. ê° ì²­í¬ íŒŒì‹±
        print('\n[ë‹¨ê³„ 4] ì²­í¬ íŒŒì‹±')
        results = self.parse_chunks(chunks)

        # 5. ê²°ê³¼ ë³‘í•©
        print('\n[ë‹¨ê³„ 5] ê²°ê³¼ ë³‘í•©')
        merged = self.merge_results(results, pdf_path.name, output_format)

        # 6. ì €ì¥
        if merged:
            # JSON ì €ì¥
            output_file = self.output_dir / f'{pdf_path.stem}_smart.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(merged, f, ensure_ascii=False, indent=2)

            print(f'\nğŸ’¾ JSON ì €ì¥: {output_file}')

            # Raw content ì €ì¥ (HTML)
            raw_file = self.output_dir / f'{pdf_path.stem}_smart.{output_format}'
            with open(raw_file, 'w', encoding='utf-8') as f:
                f.write(merged['content'])

            print(f'ğŸ’¾ Raw ì €ì¥: {raw_file}')

            # ë¯¸ë¦¬ë³´ê¸°
            print(f'\n[{output_format.upper()} ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì)]')
            print('-' * 80)
            print(merged['content'][:500])
            print('-' * 80)

        # 7. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        self.cleanup_temp_files()

        return merged


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print('=' * 80)
    print('ğŸ¥ ì§ˆë³‘ì½”ë”©ì‚¬ë¡€ì§‘ PDF ì§€ëŠ¥í˜• íŒŒì‹±')
    print('=' * 80)

    # íŒŒì‹±í•  PDF
    pdf_path = Path('data/hira_master/ì œ8ì°¨ í•œêµ­í‘œì¤€ì§ˆë³‘ì‚¬ì¸ë¶„ë¥˜ ì§ˆë³‘ì½”ë”©ì‚¬ë¡€ì§‘.pdf')

    if not pdf_path.exists():
        print(f'âŒ íŒŒì¼ ì—†ìŒ: {pdf_path}')
        return

    try:
        # ëª©í‘œ 50í˜ì´ì§€ ì²­í¬
        parser = SmartPDFSplitParser(target_chunk_pages=50)
        result = parser.parse_large_pdf(pdf_path, output_format='html')

        # ìµœì¢… ìš”ì•½
        print('\n\n' + '=' * 80)
        print('ğŸ“Š íŒŒì‹± ì™„ë£Œ')
        print('=' * 80)

        if result:
            print(f'âœ… ì´ í˜ì´ì§€: {result.get("total_pages")}p')
            print(f'âœ… ì²­í¬: {result.get("chunks_parsed")}ê°œ ì„±ê³µ, {result.get("chunks_failed")}ê°œ ì‹¤íŒ¨')
            print(f'âœ… Content: {len(result.get("content", "")):,}ì')
            print(f'âœ… Elements: {len(result.get("elements", []))}ê°œ')
            print(f'âœ… íŒŒì‹± ë°©ì‹: {result.get("parsing_method")}')
        else:
            print('âŒ íŒŒì‹± ì‹¤íŒ¨')

    except Exception as e:
        print(f'âŒ ì „ì²´ ì—ëŸ¬: {e}')
        import traceback
        traceback.print_exc()

    print('\n' + '=' * 80)
    print('âœ… ì™„ë£Œ')
    print('=' * 80)


if __name__ == '__main__':
    main()
