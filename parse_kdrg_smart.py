"""
KDRG ë¶„ë¥˜ì§‘ ì§€ëŠ¥í˜• íŒŒì‹±
- ëª©ì°¨ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ MDC ë‹¨ìœ„ë¡œ ë¶„í• 
- Upstage APIë¡œ ê° ì²­í¬ íŒŒì‹±
"""
import sys
import codecs
from pathlib import Path
import json
import os
from datetime import datetime
from dotenv import load_dotenv
from pypdf import PdfReader, PdfWriter
import requests
import time

# UTF-8 ì¶œë ¥
if sys.platform == 'win32':
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()


class SmartKDRGParser:
    """MDC êµ¬ì¡° ê¸°ë°˜ ì§€ëŠ¥í˜• KDRG íŒŒì„œ"""

    def __init__(self, output_dir: str = 'data/hira_master/kdrg_parsed'):
        self.api_key = os.getenv('UPSTAGE_API_KEY')
        if not self.api_key:
            raise ValueError('UPSTAGE_API_KEY not found in .env')

        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.base_url = "https://api.upstage.ai/v1/document-ai/document-parse"

        # ì„ì‹œ ë¶„í•  íŒŒì¼ ì €ì¥ ìœ„ì¹˜
        self.temp_dir = self.output_dir / 'temp_chunks'
        self.temp_dir.mkdir(exist_ok=True)

    def load_structure(self, structure_file: Path) -> dict:
        """ì €ì¥ëœ êµ¬ì¡° íŒŒì¼ ë¡œë“œ"""
        with open(structure_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def split_pdf(self, pdf_path: Path, chunks: list) -> list:
        """êµ¬ì¡° ê¸°ë°˜ PDF ë¶„í• """
        print(f'\nğŸ“„ ì§€ëŠ¥í˜• PDF ë¶„í• : {pdf_path.name}')

        reader = PdfReader(pdf_path)
        total_pages = len(reader.pages)

        print(f'  ì´ í˜ì´ì§€: {total_pages}p')
        print(f'  ë¶„í•  ê³„íš: {len(chunks)}ê°œ ì²­í¬\n')

        split_files = []

        for i, chunk in enumerate(chunks, 1):
            start_page = chunk['start'] - 1  # 0-indexed
            end_page = chunk['end']

            # ìƒˆ PDF ìƒì„±
            writer = PdfWriter()
            for page_num in range(start_page, end_page):
                if page_num < total_pages:
                    writer.add_page(reader.pages[page_num])

            # ì €ì¥
            chunk_filename = f'kdrg_chunk{i:02d}_p{chunk["start"]:04d}-{chunk["end"]:04d}.pdf'
            chunk_path = self.temp_dir / chunk_filename

            writer.write(chunk_path)

            split_files.append({
                'path': chunk_path,
                'chunk_info': chunk,
                'index': i
            })

            print(f'  {i:2d}. {chunk["name"]:70s} p.{chunk["start"]:4d}-{chunk["end"]:4d} ({chunk["pages"]:3d}p)')

        print(f'\n  âœ… {len(split_files)}ê°œ íŒŒì¼ ìƒì„± ì™„ë£Œ')

        return split_files

    def parse_chunk(self, chunk: dict, total_chunks: int, output_format: str = "html") -> dict:
        """ë‹¨ì¼ ì²­í¬ íŒŒì‹±"""
        chunk_path = chunk['path']
        chunk_info = chunk['chunk_info']
        index = chunk['index']

        print(f'\n[{index}/{total_chunks}] {chunk_info["name"]}')
        print(f'  íŒŒì¼: {chunk_path.name}')

        with open(chunk_path, 'rb') as f:
            files = {'document': (chunk_path.name, f, 'application/pdf')}
            data = {
                'ocr': 'true',
                'output_formats': f'["{output_format}"]',
            }

            start_time = time.time()
            try:
                response = requests.post(
                    self.base_url,
                    headers={"Authorization": f"Bearer {self.api_key}"},
                    files=files,
                    data=data,
                    timeout=300
                )
                elapsed = time.time() - start_time

                if response.status_code != 200:
                    error_msg = f"API Error {response.status_code}: {response.text}"
                    print(f'  âŒ ì‹¤íŒ¨: {error_msg}')
                    return {'error': error_msg, 'chunk': chunk_info}

                result = response.json()

                # content ì¶”ì¶œ
                content_dict = result.get('content', {})
                if isinstance(content_dict, dict):
                    content_text = content_dict.get(output_format, '')
                else:
                    content_text = str(content_dict)

                result['chunk_metadata'] = {
                    'chunk_info': chunk_info,
                    'chunk_file': str(chunk_path),
                    'elapsed_seconds': elapsed,
                    'content_length': len(content_text),
                    'api_pages': result.get('usage', {}).get('pages', 0)
                }

                print(f'  âœ… ì„±ê³µ: {elapsed:.1f}ì´ˆ, {len(content_text):,}ì')
                return result

            except Exception as e:
                print(f'  âŒ ì—ëŸ¬: {e}')
                return {'error': str(e), 'chunk': chunk_info}

    def parse_chunks(self, chunks: list) -> list:
        """ë¶„í• ëœ PDF íŒŒì¼ë“¤ íŒŒì‹±"""
        print(f'\nğŸ”„ ì²­í¬ íŒŒì‹±: {len(chunks)}ê°œ')

        results = []
        total = len(chunks)

        for chunk in chunks:
            result = self.parse_chunk(chunk, total)
            results.append(result)

            # Rate limit ë°©ì§€
            time.sleep(1)

        return results

    def merge_results(self, results: list, source_file: str, output_format: str = 'html') -> dict:
        """íŒŒì‹± ê²°ê³¼ ë³‘í•©"""
        print(f'\nğŸ”— ê²°ê³¼ ë³‘í•©')

        # ì—ëŸ¬ê°€ ì—†ëŠ” ê²°ê³¼ë§Œ í•„í„°ë§
        valid_results = [r for r in results if 'error' not in r]

        if not valid_results:
            print('  âŒ ëª¨ë“  ì²­í¬ íŒŒì‹± ì‹¤íŒ¨')
            return None

        # Content ë³‘í•©
        merged_content = []
        for r in valid_results:
            content_dict = r.get('content', {})
            if isinstance(content_dict, dict):
                content_text = content_dict.get(output_format, '')
            else:
                content_text = str(content_dict)
            merged_content.append(content_text)

        merged_content_str = '\n\n<hr>\n\n'.join(merged_content)

        # Elements ë³‘í•©
        merged_elements = []
        for r in valid_results:
            if 'elements' in r:
                merged_elements.extend(r['elements'])

        # ì´ í˜ì´ì§€ ìˆ˜
        total_pages = sum(r.get('chunk_metadata', {}).get('api_pages', 0) for r in valid_results)

        # ì²­í¬ ë©”íƒ€ë°ì´í„°
        chunks_metadata = []
        for r in valid_results:
            if 'chunk_metadata' in r:
                chunks_metadata.append(r['chunk_metadata'])

        merged = {
            'source_file': source_file,
            'total_pages': total_pages,
            'chunks_parsed': len(valid_results),
            'chunks_failed': len(results) - len(valid_results),
            'content': merged_content_str,
            'elements': merged_elements,
            'output_format': output_format,
            'parsing_method': 'mdc_structure_based_split',
            'parsed_at': datetime.now().isoformat(),
            'chunks_metadata': chunks_metadata
        }

        print(f'  âœ… ë³‘í•© ì™„ë£Œ: {total_pages}p, {len(merged_content_str):,}ì')
        print(f'  âœ… Elements: {len(merged_elements)}ê°œ')

        # ì²­í¬ë³„ ìš”ì•½
        print(f'\n  ğŸ“‹ ì²­í¬ë³„ ë¶„í• :')
        for meta in chunks_metadata:
            chunk_info = meta['chunk_info']
            print(f'     {chunk_info["name"]:70s} p.{chunk_info["start"]:4d}-{chunk_info["end"]:4d} ({chunk_info["pages"]:3d}p)')

        return merged

    def cleanup_temp_files(self):
        """ì„ì‹œ íŒŒì¼ ì‚­ì œ"""
        print(f'\nğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬')

        for file in self.temp_dir.glob('*.pdf'):
            file.unlink()
            print(f'  ì‚­ì œ: {file.name}')

    def parse_large_pdf(self, pdf_path: Path, structure_file: Path, output_format: str = 'html') -> dict:
        """ëŒ€ìš©ëŸ‰ PDF ì „ì²´ íŒŒì‹± í”„ë¡œì„¸ìŠ¤ (ì§€ëŠ¥í˜• ë¶„í• )"""
        print(f'\n{"="*80}')
        print(f'ğŸ“š ì§€ëŠ¥í˜• KDRG íŒŒì‹±: {pdf_path.name}')
        print(f'{"="*80}')

        # 1. êµ¬ì¡° ì •ë³´ ë¡œë“œ
        print('\n[ë‹¨ê³„ 1] êµ¬ì¡° ì •ë³´ ë¡œë“œ')
        structure = self.load_structure(structure_file)
        chunks = structure['suggested_chunks']

        print(f'  ì´ í˜ì´ì§€: {structure["total_pages"]}p')
        print(f'  MDC ì„¹ì…˜: {structure["mdc_count"]}ê°œ')
        print(f'  ë¶„í•  ì²­í¬: {len(chunks)}ê°œ')

        # 2. PDF ë¶„í• 
        print('\n[ë‹¨ê³„ 2] PDF ë¶„í• ')
        split_files = self.split_pdf(pdf_path, chunks)

        # 3. ê° ì²­í¬ íŒŒì‹±
        print('\n[ë‹¨ê³„ 3] ì²­í¬ íŒŒì‹±')
        results = self.parse_chunks(split_files)

        # 4. ê²°ê³¼ ë³‘í•©
        print('\n[ë‹¨ê³„ 4] ê²°ê³¼ ë³‘í•©')
        merged = self.merge_results(results, pdf_path.name, output_format)

        # 5. ì €ì¥
        if merged:
            # JSON ì €ì¥
            output_file = self.output_dir / 'kdrg_smart.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(merged, f, ensure_ascii=False, indent=2)

            print(f'\nğŸ’¾ JSON ì €ì¥: {output_file}')

            # Raw content ì €ì¥ (HTML)
            raw_file = self.output_dir / f'kdrg_smart.{output_format}'
            with open(raw_file, 'w', encoding='utf-8') as f:
                f.write(merged['content'])

            print(f'ğŸ’¾ Raw ì €ì¥: {raw_file}')

            # ë¯¸ë¦¬ë³´ê¸°
            print(f'\n[{output_format.upper()} ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì)]')
            print('-' * 80)
            print(merged['content'][:500])
            print('-' * 80)

        # 6. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        self.cleanup_temp_files()

        return merged


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print('='*80)
    print('ğŸ¥ KDRG ë¶„ë¥˜ì§‘ ì§€ëŠ¥í˜• íŒŒì‹±')
    print('='*80)

    pdf_path = Path('data/hira_master/KDRG ë¶„ë¥˜ì§‘(ì‹ í¬ê´„ì§€ë¶ˆì œë„ìš© ver1.4).pdf')
    structure_file = Path('data/hira_master/kdrg_structure.json')

    if not pdf_path.exists():
        print(f'âŒ íŒŒì¼ ì—†ìŒ: {pdf_path}')
        return

    if not structure_file.exists():
        print(f'âŒ êµ¬ì¡° íŒŒì¼ ì—†ìŒ: {structure_file}')
        print('ë¨¼ì € extract_kdrg_toc.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.')
        return

    try:
        parser = SmartKDRGParser()
        result = parser.parse_large_pdf(pdf_path, structure_file, output_format='html')

        # ìµœì¢… ìš”ì•½
        print('\n\n' + '='*80)
        print('ğŸ“Š íŒŒì‹± ì™„ë£Œ')
        print('='*80)

        if result:
            print(f'âœ… ì´ í˜ì´ì§€: {result.get("total_pages")}p')
            print(f'âœ… ì²­í¬: {result.get("chunks_parsed")}ê°œ ì„±ê³µ, {result.get("chunks_failed")}ê°œ ì‹¤íŒ¨')
            print(f'âœ… Content: {len(result.get("content", "")):,}ì')
            print(f'âœ… Elements: {len(result.get("elements", []))}ê°œ')
            print(f'âœ… íŒŒì‹± ë°©ì‹: {result.get("parsing_method")}')
        else:
            print('âŒ íŒŒì‹± ì‹¤íŒ¨')

    except Exception as e:
        print(f'âŒ ì „ì²´ ì—ëŸ¬: {e}')
        import traceback
        traceback.print_exc()

    print('\n' + '='*80)
    print('âœ… ì™„ë£Œ')
    print('='*80)


if __name__ == '__main__':
    main()
